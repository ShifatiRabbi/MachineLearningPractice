{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "IH62CjF8kNDS",
    "outputId": "a6af89d6-5b31-453c-e3ef-738cadf69eee"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data as a list of dictionaries\n",
    "data = [\n",
    "\n",
    "   {'Student_Id': 'A', 'Name': 'Oni', 'Gender': 'F', 'Session': '2018-2019', 'Department': 'CSE', 'Hall_Name': 'Jahanara Imam', 'Percentage_of_Attendance': '90%', 'Is_Alloted': 'Yes'},\n",
    "{'Student_Id': 'B', 'Name': 'Mangsura', 'Gender': 'F', 'Session': '2018-2019', 'Department': 'Math', 'Hall_Name': 'Khaleda Zia', 'Percentage_of_Attendance': '100%', 'Is_Alloted': 'Yes'},\n",
    "{'Student_Id': 'C', 'Name': 'Keya', 'Gender': 'F', 'Session': '2016-2017', 'Department': 'Chemistry', 'Hall_Name': 'Sheikh Hasina', 'Percentage_of_Attendance': '80%', 'Is_Alloted': 'No'},\n",
    "{'Student_Id': 'D', 'Name': 'Tanzila', 'Gender': 'F', 'Session': '2018-2019', 'Department': 'CSE', 'Hall_Name': 'Pritilata', 'Percentage_of_Attendance': '30%', 'Is_Alloted': 'Yes'},\n",
    "{'Student_Id': 'E', 'Name': 'Rupa', 'Gender': 'F', 'Session': '2018-2019', 'Department': 'Math', 'Hall_Name': 'Khaleda Zia', 'Percentage_of_Attendance': '85%', 'Is_Alloted': 'Yes'},\n",
    "{'Student_Id': 'F', 'Name': 'Sayma', 'Gender': 'F', 'Session': '2018-2019', 'Department': 'CSE', 'Hall_Name': 'Khaleda Zia', 'Percentage_of_Attendance': '40%', 'Is_Alloted': 'No'},\n",
    "{'Student_Id': 'G', 'Name': 'Shifati', 'Gender': 'M', 'Session': '2016-2017', 'Department': 'Chemistry', 'Hall_Name': 'MH', 'Percentage_of_Attendance': '70%', 'Is_Alloted': 'No'},\n",
    "{'Student_Id': 'H', 'Name': 'Rabbi', 'Gender': 'M', 'Session': '2019-2020', 'Department': 'CSE', 'Hall_Name': 'BBH', 'Percentage_of_Attendance': '50%', 'Is_Alloted': 'Yes'},\n",
    "{'Student_Id': 'I', 'Name': 'Rahim', 'Gender': 'M', 'Session': '2018-2019', 'Department': 'Math', 'Hall_Name': 'MH', 'Percentage_of_Attendance': '60%', 'Is_Alloted': 'Yes'},\n",
    "{'Student_Id': 'J', 'Name': 'Karim', 'Gender': 'M', 'Session': '2018-2019', 'Department': 'Math', 'Hall_Name': 'MH', 'Percentage_of_Attendance': '80%', 'Is_Alloted': 'Yes'},\n",
    "{'Student_Id': 'K', 'Name': 'Simita', 'Gender': 'F', 'Session': '2018-2019', 'Department': 'CSE', 'Hall_Name': 'Jahanara Imam', 'Percentage_of_Attendance': '90%', 'Is_Alloted': 'No'},\n",
    "{'Student_Id': 'L', 'Name': 'Oni', 'Gender': 'F', 'Session': '2016-2017', 'Department': 'Chemistry', 'Hall_Name': 'Sheikh Hasina', 'Percentage_of_Attendance': '30%', 'Is_Alloted': 'No'},\n",
    "{'Student_Id': 'M', 'Name': 'Rupa', 'Gender': 'F', 'Session': '2019-2020', 'Department': 'CSE', 'Hall_Name': 'Jahanara Imam', 'Percentage_of_Attendance': '20%', 'Is_Alloted': 'No'},\n",
    "{'Student_Id': 'N', 'Name': 'Keya', 'Gender': 'F', 'Session': '2018-2019', 'Department': 'CSE', 'Hall_Name': 'Khaleda Zia', 'Percentage_of_Attendance': '95%', 'Is_Alloted': 'Yes'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and features\n",
    "target_variable = \"Is_Alloted\"\n",
    "features = list(df.columns)\n",
    "features.remove(target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate entropy\n",
    "def entropy(data, target_variable):\n",
    "    value_counts = data[target_variable].value_counts()\n",
    "    probabilities = value_counts / len(data)\n",
    "    entropy = -sum(probabilities * np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "# Calculate information gain for a feature\n",
    "def information_gain(data, feature, target_variable):\n",
    "    entropy_before = entropy(data, target_variable)\n",
    "    weighted_entropy_after = 0\n",
    "    for value in data[feature].unique():\n",
    "        filtered_data = data[data[feature] == value]\n",
    "        probablity = len(filtered_data) / len(data)\n",
    "        weighted_entropy_after += probablity * entropy(filtered_data, target_variable)\n",
    "    return entropy_before - weighted_entropy_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pure(data, target_variable):\n",
    "    \"\"\"\n",
    "    Checks if the data at a node is pure, meaning all instances have the same class.\n",
    "    \"\"\"\n",
    "    # Get unique values of the target variable in the data\n",
    "    unique_classes = data[target_variable].unique()\n",
    "    \n",
    "    # If there is only one unique class, the data is pure\n",
    "    return len(unique_classes) == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_feature(data, features, target_variable):\n",
    "    \"\"\"\n",
    "    Selects the best feature to split the data on based on information gain.\n",
    "    \"\"\"\n",
    "    best_gain = float('-inf')\n",
    "    best_feature = None\n",
    "    \n",
    "    for feature in features:\n",
    "        gain = information_gain(data, feature, target_variable)\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_feature = feature\n",
    "    \n",
    "    return best_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeNode:\n",
    "    \"\"\"\n",
    "    Represents a node in the decision tree.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.feature = None\n",
    "        self.value = None\n",
    "        self.children = {}\n",
    "        self.label = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_values(data, feature):\n",
    "    \"\"\"\n",
    "    Returns the unique values of the specified feature in the dataset.\n",
    "    \"\"\"\n",
    "    return data[feature].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data, feature, value):\n",
    "    \"\"\"\n",
    "    Filters the dataset based on the specified feature and its value.\n",
    "    \"\"\"\n",
    "    return data[data[feature] == value]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a decision tree node\n",
    "class TreeNode:\n",
    "    def __init__(self, value=None, children=None):\n",
    "        self.value = value\n",
    "        self.children = children  # dictionary: feature values -> subtrees\n",
    "\n",
    "# Build the decision tree\n",
    "def build_decision_tree(data, features, target_variable):\n",
    "    # Check for termination conditions\n",
    "    if len(data[target_variable].unique()) == 1:\n",
    "        return TreeNode(data[target_variable].iloc[0])  # Return the majority class\n",
    "    if not features:\n",
    "        return TreeNode(data[target_variable].mode().iloc[0])  # Return the most frequent value\n",
    "\n",
    "    # Find the feature with the highest information gain\n",
    "    max_gain, best_feature = 0, None\n",
    "    for feature in features:\n",
    "        gain = information_gain(data, feature, target_variable)\n",
    "        if gain > max_gain:\n",
    "            max_gain, best_feature = gain, feature\n",
    "\n",
    "    # Check if there are no remaining features or the data is pure\n",
    "    if not features or is_pure(data, target_variable):\n",
    "        return TreeNode(data[target_variable].mode().iloc[0])  # Return the most frequent value\n",
    "\n",
    "    # Select the best feature to split on\n",
    "    best_feature = select_best_feature(data, features, target_variable)\n",
    "    root = TreeNode()\n",
    "    root.value = best_feature\n",
    "\n",
    "    # Split the data based on the best feature\n",
    "    for value in get_unique_values(data, best_feature):\n",
    "        filtered_data = filter_data(data, best_feature, value)\n",
    "        subtree = build_decision_tree(filtered_data, features.copy(), target_variable)\n",
    "        subtree.value = value\n",
    "        root.children[value] = subtree\n",
    "\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the decision tree structure (simplified)\n",
    "def print_tree(node, depth=0):\n",
    "    if node.value is not None:\n",
    "        print(f\"{' ' * depth}Feature: {node.value}\")\n",
    "        if node.children:\n",
    "            for value, subtree in node.children.items():\n",
    "                print(f\"{' ' * (depth + 2)}Value: {value}\")\n",
    "                print_tree(subtree, depth + 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the print_tree function\n",
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decision_tree_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m plot_tree(decision_tree_model, feature_names\u001b[38;5;241m=\u001b[39mfeatures, class_names\u001b[38;5;241m=\u001b[39mtarget_variable, filled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decision_tree_model' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
